# -*- coding: utf-8 -*-
"""CataractDetector.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1SgZlAl-06V4JFkcJGIz_volKqDhWI7YS

# Imports
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import cv2
from google.colab.patches import cv2_imshow
import os
import zipfile
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns

from skimage.feature import  greycomatrix, greycoprops
from sklearn import preprocessing
from sklearn.model_selection import KFold, cross_val_score, train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn import svm
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier  
from sklearn.metrics import confusion_matrix, accuracy_score  

import keras
from keras.models import Sequential,Model,load_model
from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, BatchNormalization 
from keras.callbacks import ModelCheckpoint, EarlyStopping,TensorBoard,ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications.resnet50 import ResNet50
from keras.applications.vgg16 import VGG16
from keras.optimizers import SGD, Adam, RMSprop

"""# Data Utilities

"""

class DataUtils():
    def __init__(self, path: str, file_name: str):

        #path of data
        self.path = path
        #name of zipped data file
        self.file_name = file_name
        #percentage of test data
        self.test_per = 0.2
        #path of extracted data
        self.data_path = " "
  

    def extract_data(self):
        """ 
        unzip data
        
        """

        #unzip data
        with zipfile.ZipFile(self.file_name) as zip_ref:
            zip_ref.extractall(self.path)

        #path of extracted data
        self.data_path = str(os.path.join(self.path,self.file_name[:-4]))
        
    
    def get_data(self):
        """
        Load data and set labels

        :return images: data
        :return labels: labels

        """
        #extract data
        self.extract_data()

        #list of labels
        labels = []
        #list of images
        images = []

        #set labels
        for img in os.listdir(self.data_path):
            #set labels according to name of files
            if os.path.splitext(img)[0][:-5] == 'normal':
                labels.append(0)
            else:
                labels.append(1)

            #resize images
            pic = cv2.imread(os.path.join(self.data_path,img),0)
            pic = cv2.resize(pic,(300,300))
            images.append(pic)
 
        #convert labels to numpy array
        labels = np.array(labels)

        return images, labels

    def split_data(self, input: np.ndarray, target: np.ndarray):
        """
        split data to train data and test data

        :param input: input data
        :param target: target data
        :return: test and train data
        """
        #split dataset to train data and test data
        x_train, x_test, y_train, y_test = train_test_split(input, target, test_size=self.test_per)

        return  x_train, x_test, y_train, y_test

    def data_generator(self, color_mode: str):
        """
        split data to train data and test data

        :param color_mode: type of images in data generator (grayscale or RGB)
        :return: test and train datagenerators
        """

        #check if color_mode is valid
        valid = {'rgb','grayscale'}
        if color_mode not in valid:
            raise ValueError("results: color_mode must be one of %r." % valid)
        
        #extract data
        self.extract_data()

        #Train and test data directories
        train = '/content/dataset2/TRAIN_DIR/'
        test = '/content/dataset2/TEST_DIR/'

        #Augumentation with zoom, rotation, vertical and horizental flip
        train_datagen = ImageDataGenerator(horizontal_flip=True, vertical_flip = True, zoom_range = 0.3, rotation_range=20)

        #train data generator
        train_generator = train_datagen.flow_from_directory(train,
                                                            color_mode=color_mode,
                                                            batch_size=32,
                                                            class_mode = 'categorical',
                                                            target_size=(224,224),
                                                            seed=42)
        #ImageDataGenerator for test data
        test_datagen = ImageDataGenerator()
        
        #test data generator
        test_generator = test_datagen.flow_from_directory(test,
                                                          color_mode=color_mode,
                                                          batch_size=32,
                                                          class_mode = 'categorical',
                                                          target_size=(224,224),
                                                          seed=42 )
        return train_generator,test_generator

"""# Eye Segmentation and Feature Extraction """

class ExtractFeature():
    def __init__(self, images_level: str = None):
        #content of images of dataset (face, eye or none)
        self.images_level = images_level

        #check if images_level is valid
        valid = {'eye','face', None}
        if images_level not in valid:
            raise ValueError("results: images_level must be one of %r." % valid)
        
    def detect_face(self, pictures: np.ndarray, labels: np.ndarray):
        """
        detect faces in images 

        :param pictures: dataset
        :param labels: labels
        :return detected_faces: detected faces in images
        :return face_labels: label of faces
        """
        #detect faces using haar cascade
        face_classifier = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')
        detected_faces = []
        faces_labels = []
        #detect faces in each image
        for img in range(pictures.shape[0]):
            faces = face_classifier.detectMultiScale(pictures[img], 1.3, 5)
            #extract faces
            if faces is not ():
                for (x,y,w,h) in faces:
                    detected_faces.append(pictures[img][y:y+h, x:x+w])
                    faces_labels.append(labels[img])
        ############################################################################
        #             #show detected faces
        #             cv2.rectangle(pictures[img],(x,y),(x+w,y+h),(127,0,255),2)
        #             cv2_imshow(pictures[img])
        #             cv2.waitKey(0)
        ############################################################################
        return np.array(detected_faces), np.array(faces_labels)


    def detect_eye(self, faces: np.ndarray,face_labels: np.ndarray):
        """
        detect eyes in images 

        :param faces: images
        :param face_labels: labels
        :return detected_eyes: detected eyes in images
        :return eyes_labels: label of eyes
        """
        detected_eyes = []
        eyes_labels = []
        #detect eyes using haar cascade 
        eye_classifier = cv2.CascadeClassifier('haarcascade_eye.xml')
        #detect eyes in each image        
        for face in range(faces.shape[0]):
            eyes = eye_classifier.detectMultiScale(faces[face],1.3,8)
            #extract eyces
            if eyes is not ():
                for (ex,ey,ew,eh) in eyes:
                    detected_eyes.append(faces[face][ey:ey+eh, ex:ex+ew])
                    eyes_labels.append(face_labels[face])
        ###############################################################################
        #             #show detected eyes
        #             cv2.rectangle(faces[face],(ex,ey),(ex+ew,ey+eh),(255,255,0),2)
        #             cv2_imshow(faces[face])
        #             cv2.waitKey(0)
        ###############################################################################
        return np.array(detected_eyes), np.array(eyes_labels)

      
    def SIFT_GLCM(self, eye_set: np.ndarray):
        """
        extract important features from images using SIFT and GLCM algorithms 

        :param eye_set: images of eyes
        :return features: extracted features
        """

        sift = cv2.xfeatures2d.SIFT_create()
        sift_descs = []
        glcm = []
        for e in eye_set:
            #get features using SIFT algorithm
            keypoints, descriptors = sift.detectAndCompute(e,None)
            descriptors=np.array(descriptors)
            descriptors=descriptors.flatten()

            #get features using GLCM algorithm
            e = np.array(e)
            gCoMat = greycomatrix(e, [1], [0],256,symmetric=True, normed=True)

            contrast = greycoprops(gCoMat, prop='contrast')[0][0]
            dissimilarity = greycoprops(gCoMat, prop='dissimilarity')[0][0]
            homogeneity = greycoprops(gCoMat, prop='homogeneity')[0][0]
            energy = greycoprops(gCoMat, prop='energy')[0][0]
            correlation = greycoprops(gCoMat, prop='correlation')[0][0]

            glcm.append([contrast,dissimilarity,homogeneity,energy,correlation])
            sift_descs.append(descriptors[:2304])

        #combine features in one feature array
        sift_descs = np.array(sift_descs)
        glcm = np.array(glcm)
        features = np.concatenate((sift_descs,glcm),axis=1)

        return features

    def extract_feature(self, data: np.ndarray, labels: np.ndarray):
        """
        extract features of dataset images

        :param data: dataset
        :param labels: labels
        :return feature_set: extracted features for dataset
        :return final_labels: labels
        """
        #if images in dataset contains eyes:
        if self.images_level == 'eye':
            #extract features directly from dataset
            feature_set = self.SIFT_GLCM(data) 
            final_labels = labels   
        #if images in dataset contains faces:
        elif self.images_level == 'face':
            #first detect eyes in images, then extract features from detected eyes
            e_data, e_label = self.detect_eye(self, data, labels)
            feature_set = self.SIFT_GLCM(e_data)
            final_labels = e_label
        #if images in dataset contains multiple face or content is unknown:
        else: 
            #first detect faces in images, then detect eyes in faces and then extract features from detected eyes
            f_data, f_label = self.detect_face(data, labels)
            e_data, e_label = self.detect_eye(f_data, f_label)
            feature_set = self.SIFT_GLCM(e_data)
            final_labels = e_label

        return feature_set, final_labels

"""# Model Handler"""

class ModelHandler():
    def __init__(self):
        #define VAL_STEPS as a global integer variable
        self.VAL_STEPS = 5
    
    def RF_classifier(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray):
        """
        train and evaluate Random Forest Classifier model

        :param x_train: input data for train model
        :param y_train: target data for train model
        :param x_test: validation data
        :param y_test: true value of validation data
        """
        #define model
        rf_classifier = RandomForestClassifier(n_estimators=100)
        #train model
        rf_classifier.fit(x_train,y_train)
        #evaluate model
        self.evaluate_model(rf_classifier, 'Random Forest Classifier', x_test, y_test)

    def RBF_SVM(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray):
        """
        train and evaluate RBF SVM model

        :param x_train: input data for train model
        :param y_train: target data for train model
        :param x_test: validation data
        :param y_test: true value of validation data
        """
        #define model
        rbf_svm = svm.SVC(kernel='rbf',gamma=0.001,C=10)
        #train model
        rbf_svm.fit(x_train,y_train)
        #evaluate model
        self.evaluate_model(rbf_svm, 'RBF SVM', x_test, y_test)

    def linear_SVM(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray):
        """
        train and evaluate linear SVM model

        :param x_train: input data for train model
        :param y_train: target data for train model
        :param x_test: validation data
        :param y_test: true value of validation data
        """ 
        #define model
        linear_svm = svm.SVC(kernel='linear',gamma=0.001,C=10)
        #train model
        linear_svm.fit(x_train,y_train)
        #evaluate model
        self.evaluate_model(linear_svm, 'Linear SVM', x_test, y_test)

    def k_neighbors_classifier(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray):
        """
        train and evaluate K Nearest Neighbor  model

        :param x_train: input data for train model
        :param y_train: target data for train model
        :param x_test: validation data
        :param y_test: true value of validation data
        """
        #define model
        knn = KNeighborsClassifier(n_neighbors=2, metric='minkowski', p=2)
        #train model
        knn.fit(x_train,y_train)
        #evaluate model
        self.evaluate_model(knn, 'K Nearest Neighbor', x_test, y_test)

    def logistic_regression(self, x_train: np.ndarray, y_train: np.ndarray, x_test: np.ndarray, y_test: np.ndarray):
        """
        train and evaluate Logistic Regression model

        :param x_train: input data for train model
        :param y_train: target data for train model
        :param x_test: validation data
        :param y_test: true value of validation data
        """
        #define model
        logistic_r = LogisticRegression(solver='liblinear')
        #train model
        logistic_r.fit(x_train,y_train)
        #evaluate model
        self.evaluate_model(logistic_r, 'Logistic Regression', x_test, y_test)

    def CNN_model(self, train_gen, test_gen, train=True):
        """
        train and evaluate Convolutional Neural Network model

        :param train_gen: train data generator
        :param test_gen: validation data generator
        :param train: train model or load trained model (if True: train model/ if False: load trained model)
        """
        if train:
            #first convolutional layer
            cnn_model = Sequential()
            cnn_model.add(Conv2D(32, kernel_size=(3,3),padding='same', activation='relu', input_shape = (224, 224, 1)))
            cnn_model.add(BatchNormalization())
            cnn_model.add(MaxPool2D(pool_size=(1,1)))
            cnn_model.add(Dropout(0.3))

            #2nd convolutional layer
            cnn_model.add(Conv2D(64,(5,5),padding='same', activation='relu'))
            cnn_model.add(BatchNormalization())
            cnn_model.add(MaxPool2D(pool_size = (2,2)))
            cnn_model.add(Dropout(0.2))

            #dense layers
            cnn_model.add(Flatten())
            cnn_model.add(Dense(60, activation='relu'))
            cnn_model.add(Dense(3, activation='softmax'))
            cnn_model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')

            #train model
            cnn_res = self.train_DL_models(cnn_model, 'Convolutional Neural Network', train_gen, test_gen, 20)
            #accuracy and loss of train process
            self.plot_history(cnn_res)
        else:
            #load trained model
            cnn_model = load_model('Convolutional Neural Network.h5')

        #evaluate model    
        self.evaluate_model(cnn_model, 'Convolutional Neural Network', test_gen= test_gen)
    
    def resnet_model(self, train_gen, test_gen, train=True):
        """
        train and evaluate ResNet model

        :param train_gen: train data generator
        :param test_gen: validation data generator
        :param train: train model or load trained model (if True: train model/ if False: load trained model)
        """
        if train:
            #define model
            base_model = ResNet50(weights= None, include_top=False, input_shape= (224,224,1))
            x = base_model.output
            x = GlobalAveragePooling2D()(x)
            x = Dropout(0.6)(x)
            out = Dense(3, activation= 'softmax')(x)
            resnet_model = Model(inputs = base_model.input, outputs = out)
            adam = Adam(learning_rate=0.001)
            resnet_model.compile(optimizer= adam, loss='categorical_crossentropy', metrics=['accuracy'])

            #train model
            resnet_res = self.train_DL_models(resnet_model, 'ResNet Model', train_gen, test_gen, 15)
            #accuracy and loss of train process
            self.plot_history(resnet_res)
        else:
            #load trained data
            resnet_model = load_model('ResNet Model.h5')

        #evaluate model
        self.evaluate_model(resnet_model, 'ResNet Model', test_gen = test_gen)

    def vgg16_model(self, train_gen, test_gen, train=True):
        """
        train and evaluate VGG16 model

        :param train_gen: train data generator
        :param test_gen: validation data generator
        :param train: train model or load trained model (if True: train model/ if False: load trained model)
        """
        if train:
            #define model
            base_model = VGG16(weights='vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5',
                               include_top=False, 
                               input_shape=(224,224,3,))
            vgg_model = Sequential()
            vgg_model.add(base_model)
            vgg_model.add(Flatten())
            vgg_model.add(Dropout(0.5))
            vgg_model.add(Dense(3, activation='softmax'))
            vgg_model.layers[0].trainable = False
            vgg_model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])

            #train model
            vgg_res = self.train_DL_models(vgg_model, 'VGG16 Model', train_gen, test_gen, 20)
            #accuracy and loss of train process
            self.plot_history(vgg_res)
        else:
            #load trained model
            vgg_model = load_model('VGG16 Model.h5')

        #evaluate model
        self.evaluate_model(vgg_model, 'VGG16 Model', test_gen = test_gen)

    def train_DL_models(self, model, model_name, train_gen, test_gen, epochs):
        """
        train deep learning models using data generator

        :param model: model
        :param train_gen: train data generator
        :param test_gen: validation data generator
        :return: history and result of trainig model
        """
        #set early stop condition
        stop = EarlyStopping(monitor = 'val_accuracy',
                             mode='max',
                             patience= 8,
                             verbose = 2,
                             restore_best_weights = True)

        #train data and validation data steps
        STEPS = train_gen.samples//train_gen.batch_size
        self.VAL_STEPS = test_gen.samples//test_gen.batch_size

        #train model
        results = model.fit(train_gen,
                            steps_per_epoch=STEPS,
                            epochs=epochs,
                            validation_data=test_gen,
                            validation_steps=self.VAL_STEPS,
                            callbacks=[stop])
        #save model
        model.save(model_name+'.h5')

        return results
        
    def evaluate_model(self, model, model_name, x_test=None, y_true= None, test_gen = None):
        """
        evaluate performance of model for predicting validation data 

        :param model: model
        :param model_name: model name
        :param x_test: validation data
        :param y_true: true label of validation data
        :param test_gen: validation data generator
        """
        
        if test_gen == None:
            #predict using numpy array as input
            predicted_y = model.predict(x_test)
            #labels of confusion matrix
            old_l = [0,1]
            plt_l = ['cataract','normal']

        else:
            #predict using data generator as input
            y = []
            predicted_y = model.predict(test_gen,steps = self.VAL_STEPS, batch_size = 32) 
            #get true values of validation data
            for i in range(self.VAL_STEPS):
                x_test,y_true = next(test_gen)
                y.extend(y_true)
       
            predicted_y = np.argmax(predicted_y, axis=1)
            y = np.array(y)
            y = np.argmax(y, axis=1)
            y_true = y

            #labels of confusion matrix
            old_l = [0,1,2]
            plt_l = ['severe','normal','mild']

        #accuracy of model
        acc = accuracy_score(y_true,predicted_y)
        print(model_name,' Accuracy: %.3f' % acc)

        # plot confusion matrix + accuracy
        conf = confusion_matrix(predicted_y, y_true)
        
        plt.figure(2)
        sns.heatmap(conf, annot=True, fmt='g', cmap = 'pink_r')     
        plt.xlabel('Predictions', fontsize=15)
        plt.ylabel('Actuals', fontsize=15)      
        plt.xticks(old_l,plt_l)
        plt.yticks(old_l,plt_l)
        plt.title(model_name + ' Confusion Matrix', fontsize=15)
        plt.savefig(model_name+'.jpg')
        # plt.show()
        plt.close()
        # print('_____________________________________________________')


    def plot_history(self, hist):
        """
        plot model's training process 

        :param hist: history and result of trained model
        """
        #plot loss in train process
        plt.figure(0)
        plt.plot(hist.history['loss'])
        plt.title('Loss in Train Process')
        plt.ylabel('loss')
        plt.xlabel('epoch')
        plt.savefig('history_loss.jpg')
        plt.close()

        #plot accuracy in train process
        plt.figure(1)
        plt.plot(hist.history['accuracy'])
        plt.title('Accuracy in Train Process')
        plt.ylabel('accuracy')
        plt.xlabel('epoch')
        plt.savefig('history_accuracy.jpg')
        plt.close()

"""# Run"""

data_handler1 = DataUtils("/content","dataset.zip")
pics, l= data_handler1.get_data()
feature_extractor = ExtractFeature('eye')
features_vect, features_l = feature_extractor.extract_feature(pics, l)
X_train, X_test, Y_train, Y_test = data_handler1.split_data(features_vect, features_l)

data_handler2 = DataUtils("/content","dataset2.zip")
gray_tr_gen, gray_val_gen= data_handler2.data_generator(color_mode='grayscale')
rgb_tr_gen, rgb_val_gen= data_handler2.data_generator(color_mode='rgb')

model_handler = ModelHandler()
model_handler.RF_classifier(X_train, Y_train, X_test, Y_test)
model_handler.RBF_SVM(X_train, Y_train, X_test, Y_test)
model_handler.linear_SVM(X_train, Y_train, X_test, Y_test)
model_handler.k_neighbors_classifier(X_train, Y_train, X_test, Y_test)
model_handler.logistic_regression(X_train, Y_train, X_test, Y_test)

model_handler.CNN_model(gray_tr_gen, gray_val_gen, train=False)
model_handler.resnet_model(gray_tr_gen, gray_val_gen, train=False)
model_handler.vgg16_model(rgb_tr_gen, rgb_val_gen, train=False)